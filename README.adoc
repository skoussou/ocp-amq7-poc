= POC ACTIVITIES



== STAGE 1:  Install all necessary components (templates, images/streams)

=== STAGE 1: Define Pre-Requisites 

=== STAGE 1: Activities

* * [underline]#Activity 1:# Install ImageStreams for AMQ Broker and ScaleDown controller in namespace /*openshift*/

[source, bash]
----
oc login -u system:admin
oc replace --force -f https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/amq-broker-7-image-streams.yaml -n openshift
oc replace --force -f https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/amq-broker-7-scaledown-controller-image-streams.yaml -n openshift
----

* * [underline]#Activity 2:# Install AMQ Templates in namespace *openshift*

[source, bash]
----
for template in amq-broker-72-basic.yaml \
 amq-broker-72-ssl.yaml \
 amq-broker-72-custom.yaml \
 amq-broker-72-persistence.yaml \
 amq-broker-72-persistence-ssl.yaml \
 amq-broker-72-persistence-clustered.yaml \
 amq-broker-72-persistence-clustered-ssl.yaml;
 do
  oc replace --force -f \
 https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/templates/${template} -n openshift
  done
----



=== STAGE 1: Define Success Criteria


---


== STAGE 2:  Install Broker in Clustered (SSL) and Persistent Mode (3 PODs) (This to be defined)

=== STAGE 2:  Define Pre-Requisites 
- Compatible Storage available in the cluster
- Enough Resources

=== STAGE 2: Activities

* [underline]#Activity 1:# Create namespce *amq-pocs" to host all AMQ7 required objects

[souce, bash]
----
oc login <CLUSTER-URL> -u <USERNAME> -p <PASSWORD>
oc new-project amq-pocs
----




* [underline]#Activity 2:# Create certificates for SSL access on AMQ7 Broker

** Existing certs can be found here for the secret link:certs[]
** Alternatively create new ones with script link:certs/create-ssl-amq.sh[]

[souce, bash]
----
./certs/create-ssl-amq.sh
----





* [underline]#Activity 3:# Create Service Account for the AMQ Broker deployment, Secrets based on certs and asign to SA (MAY HAVE TO DO AFTER INSTALLATION OF OBJETS so SA exists)
** You can run also sript from echo link:scripts/setup_sa_secrets_assign.sh[]
[souce, bash]
----
echo '{"kind": "ServiceAccount", "apiVersion": "v1", "metadata": {"name": "amq-service-account"}}' | oc create -f -

# Add the view role to the service account. The view role enables the service account to view all the resources in the amq-demo namespace, which is necessary for managing the cluster when using the OpenShift dns-ping protocol for discovering the mesh endpoints.
oc policy add-role-to-user view system:serviceaccount:amq-pocs:amq-service-account

# Use the broker keystore file to create the AMQ Broker secret:
# oc secrets new amq-app-secret broker.ks
oc create secret generic amq-app-secret --from-file=./certs/broker.ks --from-file=./certs/broker.ts

# Add the secret to the service account created earlier:
oc secrets add sa/amq-service-account secret/amq-app-secret
----





* [underline]#Activity 4a:# Deploying a basic broker with persistence and SSL
* 

* [underline]#Activity 4b:# Deploying a broker with custom configuration
* 


* [underline]#Activity 4c:# Deploying a set of clustered SSL brokers
** Create AMQ7 Broker Application with Stateful Sets utilizing template "" and parameters link:docs/amq-broker-72-persistence-clustered-ssl.adoc[]

[source, bash]
----
oc new-app --template=amq-broker-72-persistence-clustered-ssl \
    -p=APPLICATION_NAME=broker \
    -p=AMQ_ROLE=admin \
    -p=AMQ_NAME=broker \
    -p=AMQ_REPLICAS="0" \
    -p=AMQ_SECRET=amq-app-secret \
    -p=AMQ_TRUSTSTORE=broker.ts \
    -p=AMQ_KEYSTORE=broker.ks \
    -p=AMQ_DATA_DIR=/opt/amq/data \
    -p=AMQ_DATA_DIR_LOGGING="true" \
    -p=IMAGE=registry.access.redhat.com/amq-broker-7/amq-broker-72-openshift:1.1 \
    -p=AMQ_PROTOCOL=openwire,amqp,stomp,mqtt,hornetq \
    -p=AMQ_QUEUES=demoQueue \
    -p=AMQ_ADDRESSES=demoTopic \
    -p=VOLUME_CAPACITY=1Gi \
    -p=AMQ_CLUSTERED="true" \
    -p=AMQ_USER=amq-demo-user \
    -p=AMQ_PASSWORD=amqDemoPassword \
    -p=AMQ_TRUSTSTORE_PASSWORD=broker \
    -p=AMQ_KEYSTORE_PASSWORD=broker \
    -n amq-pocs
----

  ** Scale up the pods to three to create a cluster of brokers.

[source, bash]
----  
oc scale statefulset broker-amq --replicas=3
----

  ** and verify that pods are running

[source, bash]
----  
oc get pods

NAME           READY     STATUS    RESTARTS   AGE
broker-amq-0   1/1       Running   0          33m
broker-amq-1   1/1       Running   0          33m
broker-amq-2   1/1       Running   0          29m
----

  ** Verify the brokers have clustered with the new pod by checking the logs.

[source, bash]
----  
sh-4.2$ jboss-amq-7-broker-openshift-image]$ oc logs broker-amq-2
----

* [underline]#Activity 5:# Create an SSL Route
** link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/deploying_amq_broker_on_openshift_container_platform/#creating-route-ocp_broker-ocp[3.3. Creating an SSL route]




8.5.3. Creating a route

Create a route for the broker so that clients outside of OpenShift Container Platform can connect using SSL. By default, the broker protocols are available through the 61617/TCP port.
Note

Only one broker can be scaled up. You cannot scale up multiple brokers.

Procedure

    From the Services menu choose broker-amq-tcp-ssl
    From the Action menu and choose Create a route .
    Select the Secure route check box to display the TLS parameters.
    From the TLS Termination drop-down menu, choose Passthrough. This selection relays all communication to AMQ Broker without the OpenShift router decrypting and resending it.

    View the route by going to the routes menu. For example:

    https://broker-amq-tcp-amq-demo.router.default.svc.cluster.local

This hostname will be used by external clients to connect to the broker using SSL with SNI.

Additional resources

    For more information on routes in the OpenShift Container Platform, see Routes. 



* [underline]#Activity 5:# Creating a route for the management console





8.6.4. Creating a route for the management console

The clustering templates do not expose the console by default. This is because the the OpenShift proxy would load balance around each broker in the cluster and it would not be possible to control which broker console is connected.
Note

In future releases each pod will have its own integrated console available through the use of the pod. It uses wildcard routing to expose each broker on its own hostname.

Procedure

    Choose import YAML/JSON from Add to Project drop down

    Enter the following and click create:

    apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: broker-amq
        application: broker-amq
      name: console-jolokia
    spec:
      port:
        targetPort: console-jolokia
      to:
        kind: Service
        name: broker-amq-headless
        weight: 100
      wildcardPolicy: Subdomain
      host: star.broker-amq-headless.amq-demo.svc

    Note

    The important configuration here is host: star.broker-amq-headless.amq-demo.svc. This is the hostname used for each pod in the broker. The star is replaced by the pod name, so if the pod name is broker-amq-0 , the hostname is broker-amq-0.broker-amq-headless.amq-demo.svc

    Add an entry into your /etc/hosts file to map the route name onto the IP address of the OpenShift cluster:

    10.0.0.1 broker-amq-0.broker-amq-headless.amq-demo.svc

    Navigate to the console using the address http://broker-amq-0.broker-amq-headless.amq-demo.svc in a browser. 









=== STAGE 2:  Define Success Criteria
- Define Destinations (check they are there) Topics/Queues
- Scale-Down controller has to be installed as well to monitor PVCs


---


== STAGE 3:  Client/Consumers for ingestion of data

=== STAGE 3:  Define Pre-Requisites 
- 

=== STAGE 3: Activities


* [underline]#Activity 1:# Install ScaleDown controller in namespace *amq-pocs*

[source, bash]
----
oc create -n amq-pocs -f https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/templates/amq-broker-72-persistence-clustered-controller.yaml
deployment.apps/amq-broker-72-scaledown-controller-openshift-deployment created
serviceaccount/amq-broker-72-scaledown-controller-openshift-sa created
role.rbac.authorization.k8s.io/amq-broker-72-scaledown-controller-openshift-role created
rolebinding.rbac.authorization.k8s.io/amq-broker-72-scaledown-controller-openshift-rb created
----

- 

=== STAGE 3:  Define Success Criteria

- 



---


== STAGE 4:  Setup AMQ & OCP Objects for HA & Scale Up/Downs

=== STAGE 3:  Define Pre-Requisites 
* 

=== STAGE 3: Activities

* [underline]#Activity 1:# Installing Scaledown Controller
** link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/deploying_amq_broker_on_openshift_container_platform/#install-journal-recovery-broker-ocp[7.1. Installing the scaledown controller]


* [underline]#Activity 2:# Configure ScaleDown Controller
** link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/deploying_amq_broker_on_openshift_container_platform/#using_pod_draining_broker-ocp[7.2. Using the scaledown controller]

* [underline]#Activity 3:# 



=== STAGE 3:  Define Success Criteria

- 




